{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxsBR5GoviFMiMzKaXek69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xavirubi/tappx-challenge/blob/master/hiring_project/kw_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVL79AKfkQGv"
      },
      "outputs": [],
      "source": [
        "!pip install -U keybert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U rake-nltk"
      ],
      "metadata": {
        "id": "jdhdwsgEAIRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U yake"
      ],
      "metadata": {
        "id": "DeWWVDL1Y3t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "metadata": {
        "id": "NWGPuBpznMle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "S_-JJvgxDt-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_lg"
      ],
      "metadata": {
        "id": "JBgUx4ceEvwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# from keybert import KeyBERT\n",
        "# from rake_nltk import Rake\n",
        "import nltk\n",
        "# import yake\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9q7g2rJMkV-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_articles = open('articles.json')\n",
        "f_videos = open('videos.json')\n",
        "\n",
        "# articles_data = json.load(f_articles)\n",
        "# videos_data = json.load(f_videos)\n",
        "\n",
        "article_ids = list(article_id for article_id in articles_data.keys())\n",
        "video_ids = list(video_id for video_id in videos_data.keys())"
      ],
      "metadata": {
        "id": "sfCcPwN3KVoU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def article_text(id):\n",
        "\treturn articles_data[id][\"text\"]\n",
        "\n",
        "def video_text(id):\n",
        "\treturn videos_data[id][\"text\"]\n",
        "\n",
        "def article_keywords(id):\n",
        "\treturn articles_data[id][\"keywords\"]\n",
        "\n",
        "def video_keywords(id):\n",
        "\treturn videos_data[id][\"keywords\"]"
      ],
      "metadata": {
        "id": "HLuiT4stKzZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT()\n",
        "def keybert_keywords(text):\n",
        "  return kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3))"
      ],
      "metadata": {
        "id": "1FUsvWjVKnG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rake_model = Rake()\n",
        "def rake_keywords(text):\n",
        "  kw_extracted = rake_model.extract_keywords_from_text(text)\n",
        "  return rake_model.get_ranked_phrases_with_scores()"
      ],
      "metadata": {
        "id": "A2Xf4od5BWHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yake_keywords(text):\n",
        "  kw_extractor = yake.KeywordExtractor(lan='en', n=3)\n",
        "  return kw_extractor.extract_keywords(text)"
      ],
      "metadata": {
        "id": "5VYG-4faZJD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in article_ids:\n",
        "  print(f'''TEXT FROM ARTICLE {id}:\n",
        "  {article_text(id)}\n",
        "''')\n",
        "  print(f'''KEYWORDS FROM ARTICLE {id}:\n",
        "  {article_keywords(id)}\n",
        "''')\n",
        "  print(f'''KEYBERT KEYWORDS FROM ARTICLE {id}:\n",
        "{keybert_keywords(article_text(id))}\n",
        "''')\n",
        "\n",
        "  print(f'''RAKE KEYWORDS FROM ARTICLE {id}:\n",
        "{rake_keywords(article_text(id))}\n",
        "''')\n",
        "\n",
        "  print(f'''YAKE KEYWORDS FROM ARTICLE {id}:\n",
        "{yake_keywords(article_text(id))}\n",
        "\n",
        "\n",
        "\n",
        "-----------------------------------------------------------------------------------------------------\n",
        "''')\n",
        "\n",
        "f_articles.close()"
      ],
      "metadata": {
        "id": "dPkA4OBHLNrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1 = article_keywords(article_ids[0])\n",
        "# corpus2 = article_keywords(article_ids[7])\n",
        "# print(f\"CORPUS 1: {corpus1}\\n\\nCORPUS 2: {corpus2}\\n--------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "model = Word2Vec(corpus1, window=5, min_count=1, workers=4)\n",
        "\n",
        "model = Word2Vec(corpus1)"
      ],
      "metadata": {
        "id": "sC66ihTYnzB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(video_ids)):\n",
        "  corpus2 = video_keywords(video_ids[i])\n",
        "  similarity_score = model.wv.n_similarity(corpus1, corpus2)\n",
        "  print(f\"SIMILARITY SCORE: {similarity_score}\\n\")"
      ],
      "metadata": {
        "id": "kiU5g4w4osW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "doc1 = nlp(article_text(article_ids[1]))\n",
        "# doc2 = nlp(video_text(video_ids[0]))\n",
        "print(article_keywords(article_ids[1]))\n",
        "# Similarity of two documents\n",
        "for i in range(0, len(video_ids)):\n",
        "  doc2 = nlp(video_text(video_ids[i]))\n",
        "  print(i, doc1.similarity(doc2), video_keywords(video_ids[i]))\n"
      ],
      "metadata": {
        "id": "hgFlLNnwHxE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(article_keywords(article_ids[0]))\n",
        "\n",
        "def score_func(article_kws, video_kws):\n",
        "  score = 0\n",
        "  for article_kw in article_kws:\n",
        "    for video_kw in video_kws:\n",
        "      score += nlp(article_kw).similarity(nlp(video_kw))\n",
        "  return score\n",
        "\n",
        "for i in range(0, len(video_ids)):\n",
        "  score = score_func(article_keywords(article_ids[0]), video_keywords(video_ids[i]))\n",
        "  print(i, score, video_keywords(video_ids[i]), end='\\n\\n')\n"
      ],
      "metadata": {
        "id": "zPIp0wuiMFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")\n"
      ],
      "metadata": {
        "id": "dxgTNWcDod_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = article_keywords(article_ids[0])\n",
        "\n",
        "for i in range(0, len(video_ids)):\n",
        "  sequence_to_classify = video_text(video_ids[i])\n",
        "  print(classifier(sequence_to_classify, candidate_labels, multi_label=True))\n",
        "\n",
        "#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
        "# 'scores': [0.9945111274719238,\n",
        "#  0.9383890628814697,\n",
        "#  0.0057061901316046715,\n",
        "#  0.0018193122232332826],\n",
        "# 'sequence': 'one day I will see the world'}\n"
      ],
      "metadata": {
        "id": "cl0_DVvfo2kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "DVBsEkmTJVxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "47qUvI6fH03_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def article_keywords_to_string(article: Dict) -> str:\n",
        "    return \" \".join(article['keywords'])\n",
        "\n",
        "def video_keywords_to_string(video: Dict) -> str:\n",
        "    return \" \".join(video['keywords'])"
      ],
      "metadata": {
        "id": "cwKhY5bsIE3B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_videos_to_articles(articles: List[Dict], videos: List[Dict], threshold: float = 0.5) -> Dict[int, List[int]]:\n",
        "    article_keywords = [article_keywords_to_string(article) for article in articles]\n",
        "    video_keywords = [video_keywords_to_string(video) for video in videos]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    article_matrix = vectorizer.fit_transform(article_keywords)\n",
        "    video_matrix = vectorizer.transform(video_keywords)\n",
        "\n",
        "    similarity_matrix = cosine_similarity(article_matrix, video_matrix)\n",
        "\n",
        "    related_videos = defaultdict(list)\n",
        "    for article_index, article_row in enumerate(similarity_matrix):\n",
        "        for video_index, similarity_score in enumerate(article_row):\n",
        "            if similarity_score >= threshold:\n",
        "                related_videos[article_index].append(video_index)\n",
        "\n",
        "    return related_videos"
      ],
      "metadata": {
        "id": "sNUckfVeIO1f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_articles = open('articles.json')\n",
        "f_videos = open('videos.json')\n",
        "article_data = json.load(f_articles)\n",
        "video_data = json.load(f_videos)\n",
        "\n",
        "list_of_articles = list(value for value in article_data.values())\n",
        "list_of_videos = list(value for value in video_data.values())\n",
        "\n",
        "related_videos = assign_videos_to_articles(list_of_articles, list_of_videos)\n",
        "\n",
        "for article_index, video_indices in related_videos.items():\n",
        "    print(f\"Article {article_index}: {list_of_articles[article_index]['keywords']}\")\n",
        "    for video_index in video_indices:\n",
        "      print(f\"  Related Video: {list_of_videos[video_index]['keywords']}\")\n"
      ],
      "metadata": {
        "id": "HFdCspLIJBg5",
        "outputId": "fea64ec6-0ea4-4705-9f17-4391175c686e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 6: ['Recetas navidenas en freidora de aire', 'Solomillo Wellington', 'duxelle de champinones', 'Pollo de Navidad', 'zumo de naranja', 'aceite de oliva', 'condimento italiano', 'ramas de romero', 'uvas negras', 'Panceta de cerdo con manzanas asadas', 'jarabe de arce', 'vinagre de sidra de manzana', 'azucar', 'mostaza', 'especias italianas', 'judias verdes al vapor', 'corteza de cerdo', 'sal marina', 'pulpa tierna y bien hecha.']\n",
            "  Related Video: ['iniciacion guitarra', 'acordes basicos', 'diagramas de acordes', 'partes de la guitarra', 'trastes', 'cuerdas', 'acorde de dos', 'leer diagramas de acordes', 'acorde de re mayor', 'acorde de la esclava']\n",
            "Article 7: ['barcos papel', 'origami', 'manualidades', 'ninos', 'estanque', 'piscina', 'agua', 'figura', 'tecnicas', 'hoja', 'pliegues', 'bordes', 'centro', 'pentagono', 'sombrero papel', 'triangulo', 'solapas', 'palas laterales', 'barco papel pirata', 'papel origami', 'color', 'tijeras', 'lapiz', 'rectangulo', 'X', 'sobre', 'casita china', 'picos interiores', 'casita china', 'pliegues verticales', 'proa', 'popa', 'mastil', 'vela', 'cartulina', 'palito de brocheta', 'silicona caliente', 'mascaron', 'velas', 'bolita de plastilina.']\n",
            "  Related Video: ['margaritas de papel', 'manualidad', 'papel de colores', 'regla', 'lapiz', 'tijeras', 'pegamento de papel', 'cortes', 'tiras de papel', 'margen', 'medidas', 'extremos', 'enrollar', 'flor', 'decorar', 'compartir', 'suscribirse', 'canal', 'tutorial.']\n"
          ]
        }
      ]
    }
  ]
}